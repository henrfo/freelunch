{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trajectory forecasting with adaptive updating "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main Features**\n",
    "- Brent Crude, WTI Crude, Dutch TTF Gas, Henry Hub Gas\n",
    "- Equinor (EQNR.OL): Open, Close, High, Low, Volume, Market Cap\n",
    "- OSEBX Index: Open, Close, High, Low, Volume\n",
    "- VIX (volatility index)\n",
    "- Dollar Index (DXY)\n",
    "\n",
    "**Relevant Stocks**\n",
    "- **Norway**: Aker BP (AKRBP), DNO (DNO), Vår Energi (VAR), Petroleum Geo-Services (PGS), BW Offshore (BWO), Frontline (FRO)\n",
    "- **US/Global**: Exxon (XOM), Chevron (CVX), Shell (SHEL), BP (BP), TotalEnergies (TTE), ConocoPhillips (COP), Occidental (OXY)\n",
    "\n",
    "**Stock Exchanges**\n",
    "- S&P 500, NASDAQ, Dow Jones\n",
    "- FTSE 100, DAX, CAC 40\n",
    "- Nikkei 225, Hang Seng\n",
    "\n",
    "**Commodity Prices**\n",
    "- Gold (XAU), Silver (XAG)\n",
    "- **Currencies**: USD/NOK, EUR/NOK, GBP/NOK, SEK/NOK, USD/EUR\n",
    "- Coal (API2), Uranium (UX)\n",
    "- Carbon Credits (EU ETS)\n",
    "\n",
    "**Economic Indicators**\n",
    "- **Interest Rates**: Norway (Norges Bank), US Fed Funds, ECB, BoE, BoJ, PBoC\n",
    "- **Inflation**: Norway CPI, US CPI, EU HICP\n",
    "- **Unemployment**: Norway, US, EU rates\n",
    "- **Analyst Targets**: Equinor consensus price targets, EPS estimates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data fetching\n",
    "import yfinance as yf\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ML libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Data preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Date and time handling\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: COLLECT DATA\n",
    "\n",
    "def collect_data(start_date=\"2021-01-01\", end_date=None):\n",
    "    \"\"\"Collect stock data and return as DataFrame\"\"\"\n",
    "    end_date = end_date or datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Updated tickers based on research\n",
    "    tickers = {\n",
    "        # Main stock\n",
    "        'EQNR.OL': 'equinor',\n",
    "       \n",
    "        # Energy commodities\n",
    "        'BZ=F': 'brent_crude',\n",
    "        'CL=F': 'wti_crude',\n",
    "        'TTF=F': 'ttf_gas',\n",
    "        'NG=F': 'henry_hub',\n",
    "       \n",
    "        # Norwegian energy stocks\n",
    "        'AKRBP.OL': 'aker_bp',\n",
    "        'DNO.OL': 'dno',\n",
    "        'VAR.OL': 'var_energi',\n",
    "        'PGS.OL': 'pgs',\n",
    "        'BWO.OL': 'bw_offshore',\n",
    "        'FRO.OL': 'frontline',\n",
    "       \n",
    "        # Global energy stocks\n",
    "        'XOM': 'exxon',\n",
    "        'CVX': 'chevron',\n",
    "        'SHEL': 'shell',\n",
    "        'BP': 'bp',\n",
    "        'TTE': 'totalenergies',\n",
    "        'COP': 'conocophillips',\n",
    "        'OXY': 'occidental',\n",
    "       \n",
    "        # Indices\n",
    "        'OSEBX.OL': 'osebx',\n",
    "        '^GSPC': 'sp500',\n",
    "        '^IXIC': 'nasdaq',\n",
    "        '^DJI': 'dow_jones',\n",
    "        '^FTSE': 'ftse100',\n",
    "        '^GDAXI': 'dax',\n",
    "        '^FCHI': 'cac40',\n",
    "        '^N225': 'nikkei',\n",
    "        '^HSI': 'hang_seng',\n",
    "       \n",
    "        # Volatility and Dollar\n",
    "        '^VIX': 'vix',\n",
    "        'DX-Y.NYB': 'dollar_index',\n",
    "       \n",
    "        # Commodities\n",
    "        'GC=F': 'gold',\n",
    "        'SI=F': 'silver',\n",
    "       \n",
    "        # Currencies\n",
    "        'NOK=X': 'usd_nok',\n",
    "        'EURNOK=X': 'eur_nok',\n",
    "        'GBPNOK=X': 'gbp_nok',\n",
    "        'SEKNOK=X': 'sek_nok',\n",
    "        'EURUSD=X': 'eur_usd'\n",
    "    }\n",
    "    \n",
    "    all_data = {}\n",
    "    \n",
    "    # Download each ticker separately to avoid alignment issues\n",
    "    for ticker, name in tickers.items():\n",
    "        try:\n",
    "            print(f\"Downloading {name}...\")\n",
    "            # Download individually\n",
    "            data = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "            \n",
    "            if len(data) > 0:\n",
    "                # Only keep OHLC and Volume columns\n",
    "                cols_to_keep = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "                data = data[[c for c in cols_to_keep if c in data.columns]]\n",
    "                # Handle column renaming - columns might be strings or tuples\n",
    "                new_cols = []\n",
    "                for col in data.columns:\n",
    "                    if isinstance(col, tuple):\n",
    "                        col_name = col[0] if len(col) > 0 else str(col)\n",
    "                    else:\n",
    "                        col_name = str(col)\n",
    "                    new_cols.append(f\"{name}_{col_name.lower()}\")\n",
    "                data.columns = new_cols\n",
    "                all_data[name] = data\n",
    "                print(f\"  ✓ {name}: {len(data)} rows\")\n",
    "            else:\n",
    "                print(f\"  ✗ No data received for {ticker}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error fetching {ticker}: {e}\")\n",
    "    \n",
    "    if not all_data:\n",
    "        print(\"No data collected\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Combine using outer join to keep all dates\n",
    "    df = pd.concat(all_data.values(), axis=1, join='outer')\n",
    "    print(f\"Combined data: {len(df)} rows, {len(df.columns)} columns\")\n",
    "    \n",
    "    # Check initial NaN percentage\n",
    "    nan_pct = df.isnull().sum().sum() / df.size * 100\n",
    "    print(f\"Initial NaN percentage: {nan_pct:.2f}%\")\n",
    "    \n",
    "    # Keep only dates where Equinor traded (removes weekends/holidays)\n",
    "    if 'equinor_close' in df.columns:\n",
    "        before_filter = len(df)\n",
    "        df = df[df['equinor_close'].notna()]\n",
    "        print(f\"Filtered to Equinor trading days: {before_filter} → {len(df)} rows\")\n",
    "    \n",
    "    # Forward fill then backward fill to handle gaps\n",
    "    df = df.ffill().bfill()\n",
    "    \n",
    "    # For any remaining NaNs at the beginning, drop those rows\n",
    "    # This happens when some tickers start trading later than others\n",
    "    first_valid_idx = df.first_valid_index()\n",
    "    last_valid_idx = df.last_valid_index()\n",
    "    if first_valid_idx and last_valid_idx:\n",
    "        df = df.loc[first_valid_idx:last_valid_idx]\n",
    "    \n",
    "    # Final check for NaN percentage\n",
    "    nan_count = df.isnull().sum().sum()\n",
    "    if nan_count > 0:\n",
    "        nan_pct_final = nan_count / df.size * 100\n",
    "        print(f\"Warning: {nan_count} NaN values remain ({nan_pct_final:.2f}%)\")\n",
    "        # Show which columns have NaNs\n",
    "        nan_cols = df.columns[df.isnull().any()].tolist()\n",
    "        if nan_cols:\n",
    "            print(f\"  Columns with NaNs: {nan_cols}\")\n",
    "    \n",
    "    print(f\"\\nFinal data: {len(df)} rows, {len(df.columns)} columns\")\n",
    "    if len(df) > 0:\n",
    "        print(f\"Date range: {df.index[0].date()} to {df.index[-1].date()}\")\n",
    "        nan_pct_final = df.isnull().sum().sum() / df.size * 100\n",
    "        print(f\"Final NaN percentage: {nan_pct_final:.2f}%\")\n",
    "    else:\n",
    "        print(\"WARNING: No data remaining after processing\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run collection\n",
    "data = collect_data(start_date=\"2015-01-01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rate of Change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add rate of change features\n",
    "def add_rate_of_change(df):\n",
    "    \"\"\"Add rate of change features to existing dataframe\"\"\"\n",
    "    \n",
    "    # Key assets for rate of change analysis\n",
    "    key_assets = ['equinor', 'brent_crude', 'wti_crude', 'usd_nok', 'vix']\n",
    "    \n",
    "    for asset in key_assets:\n",
    "        close_col = f'{asset}_close'\n",
    "        if close_col in df.columns:\n",
    "            # First derivative (daily returns)\n",
    "            df[f'{asset}_return'] = df[close_col].pct_change()\n",
    "            \n",
    "            # Second derivative (acceleration)\n",
    "            df[f'{asset}_acceleration'] = df[f'{asset}_return'].diff()\n",
    "            \n",
    "            # Volatility momentum (20-day rolling vol change)\n",
    "            rolling_vol = df[f'{asset}_return'].rolling(20).std()\n",
    "            df[f'{asset}_vol_momentum'] = rolling_vol.pct_change()\n",
    "    \n",
    "    # Cross-asset correlation momentum (Equinor vs Brent)\n",
    "    if 'equinor_return' in df.columns and 'brent_crude_return' in df.columns:\n",
    "        rolling_corr = df['equinor_return'].rolling(20).corr(df['brent_crude_return'])\n",
    "        df['eq_brent_corr_momentum'] = rolling_corr.diff()\n",
    "    \n",
    "    # Drop initial NaN rows created by calculations\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to your data\n",
    "data = add_rate_of_change(data)\n",
    "print(f\"After adding rate of change: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CELL 2: PRINT HEAD OF DATA\n",
    "print(f\"Shape: {data.shape}\")\n",
    "print(f\"Date range: {data.index.min().date()} to {data.index.max().date()}\")\n",
    "print(f\"Columns: {len(data.columns)}\")\n",
    "print(f\"Remaining NaNs: {data.isnull().sum().sum()}\")\n",
    "print(f\"NaN percentage: {data.isnull().sum().sum() / data.size * 100:.2f}%\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Matrix to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CELL 3: SAVE TO CSV\n",
    "filepath = \"data/equinor_data_8sept.csv\"\n",
    "data.to_csv(filepath)\n",
    "print(f\"Saved {len(data)} rows to {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "============================================================\n",
      "ENHANCED TRAJECTORY MODEL WITH SHARPER PREDICTIONS\n",
      "============================================================\n",
      "\n",
      "1. Loading and preparing enhanced data...\n",
      "   Found 36 assets: ['nikkei', 'vix', 'wti_crude', 'dax', 'nasdaq']...\n",
      "   Created 1960 features dynamically\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-09 00:13:03,862] A new study created in memory with name: no-name-f2d6258f-f7f4-4118-965e-d5a914b52483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Features: 1960\n",
      "   Samples: 2642\n",
      "\n",
      "2. Running Optuna hyperparameter optimization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b0674fa32248149288144d6c74830b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enhanced Trajectory Distribution Model with Optuna Optimization\n",
    "Includes domain-specific features, rank predictions, and sharper loss functions\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ENHANCED DATA PREPARATION WITH DOMAIN FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_distribution_data(filepath='data/equinor_data_8sept.csv'):\n",
    "    \"\"\"Dynamically prepare data with domain-specific features\"\"\"\n",
    "    data = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
    "    \n",
    "    # Calculate next-day targets\n",
    "    data['next_day_return'] = data['equinor_close'].pct_change(1).shift(-1)\n",
    "    data['next_day_volatility'] = data['equinor_close'].pct_change(1).rolling(20).std().shift(-1)\n",
    "    \n",
    "    # Store original columns\n",
    "    original_columns = set(data.columns)\n",
    "    \n",
    "    # Detect all assets dynamically\n",
    "    price_columns = [col for col in data.columns if 'close' in col.lower()]\n",
    "    assets = list(set([col.replace('_close', '') for col in price_columns]))\n",
    "    \n",
    "    print(f\"   Found {len(assets)} assets: {assets[:5]}...\")\n",
    "    \n",
    "    # Generate features for each asset\n",
    "    for asset in assets:\n",
    "        base_col = f'{asset}_close' if f'{asset}_close' in data.columns else None\n",
    "        \n",
    "        if base_col and base_col in data.columns:\n",
    "            # Returns at multiple horizons\n",
    "            for period in [1, 2, 3, 5, 7, 10, 15, 20, 30, 60]:\n",
    "                data[f'{asset}_ret_{period}d'] = data[base_col].pct_change(period)\n",
    "            \n",
    "            # Log returns\n",
    "            data[f'{asset}_logret_1d'] = np.log(data[base_col] / data[base_col].shift(1))\n",
    "            \n",
    "            # Volatility features\n",
    "            for period in [5, 10, 20, 30, 60]:\n",
    "                data[f'{asset}_vol_{period}d'] = data[f'{asset}_ret_1d'].rolling(period).std()\n",
    "                data[f'{asset}_vol_skew_{period}d'] = data[f'{asset}_ret_1d'].rolling(period).skew()\n",
    "                data[f'{asset}_vol_kurt_{period}d'] = data[f'{asset}_ret_1d'].rolling(period).kurt()\n",
    "            \n",
    "            # Moving averages\n",
    "            for period in [5, 10, 20, 50, 100, 200]:\n",
    "                if len(data) >= period:\n",
    "                    data[f'{asset}_ma{period}'] = data[base_col].rolling(period).mean()\n",
    "                    data[f'{asset}_rel_ma{period}'] = data[base_col] / data[f'{asset}_ma{period}'] - 1\n",
    "            \n",
    "            # Technical indicators\n",
    "            for period in [5, 10, 20]:\n",
    "                data[f'{asset}_min_{period}d'] = data[base_col].rolling(period).min()\n",
    "                data[f'{asset}_max_{period}d'] = data[base_col].rolling(period).max()\n",
    "                data[f'{asset}_range_{period}d'] = (data[base_col] - data[f'{asset}_min_{period}d']) / \\\n",
    "                                                   (data[f'{asset}_max_{period}d'] - data[f'{asset}_min_{period}d'] + 1e-10)\n",
    "            \n",
    "            # RSI\n",
    "            delta = data[base_col].diff()\n",
    "            gain = (delta.where(delta > 0, 0)).rolling(14).mean()\n",
    "            loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "            rs = gain / (loss + 1e-10)\n",
    "            data[f'{asset}_rsi'] = 100 - (100 / (1 + rs))\n",
    "            \n",
    "            # MACD\n",
    "            exp1 = data[base_col].ewm(span=12, adjust=False).mean()\n",
    "            exp2 = data[base_col].ewm(span=26, adjust=False).mean()\n",
    "            data[f'{asset}_macd'] = exp1 - exp2\n",
    "            data[f'{asset}_macd_signal'] = data[f'{asset}_macd'].ewm(span=9, adjust=False).mean()\n",
    "            \n",
    "            # Bollinger Bands\n",
    "            ma20 = data[base_col].rolling(20).mean()\n",
    "            std20 = data[base_col].rolling(20).std()\n",
    "            data[f'{asset}_bb_upper'] = ma20 + (std20 * 2)\n",
    "            data[f'{asset}_bb_lower'] = ma20 - (std20 * 2)\n",
    "            data[f'{asset}_bb_pos'] = (data[base_col] - data[f'{asset}_bb_lower']) / \\\n",
    "                                      (data[f'{asset}_bb_upper'] - data[f'{asset}_bb_lower'] + 1e-10)\n",
    "    \n",
    "    # DOMAIN-SPECIFIC FEATURES FOR EQUINOR\n",
    "    if all(col in data.columns for col in ['brent_crude_ret_1d', 'usd_nok_ret_1d']):\n",
    "        # Oil-equity signal\n",
    "        data['oil_equity_signal'] = (\n",
    "            data.get('brent_crude_ret_1d', 0) * 0.5 +\n",
    "            data.get('osebx_ret_1d', 0) * 0.3 +\n",
    "            data.get('usd_nok_ret_1d', 0) * -0.2  # NOK strength = bad for Equinor\n",
    "        )\n",
    "        \n",
    "        # Oil-equity divergence\n",
    "        data['oil_equity_divergence'] = (\n",
    "            data.get('brent_crude_ret_5d', 0) - data.get('equinor_ret_5d', 0)\n",
    "        )\n",
    "    \n",
    "    # Energy sector momentum\n",
    "    energy_assets = ['aker_bp', 'var_energi', 'dno', 'equinor']\n",
    "    energy_returns = []\n",
    "    for asset in energy_assets:\n",
    "        ret_col = f'{asset}_ret_1d'\n",
    "        if ret_col in data.columns:\n",
    "            energy_returns.append(data[ret_col])\n",
    "    \n",
    "    if energy_returns:\n",
    "        data['energy_momentum'] = pd.concat(energy_returns, axis=1).mean(axis=1)\n",
    "        data['energy_momentum_5d'] = pd.concat([data[f'{asset}_ret_5d'] \n",
    "                                                for asset in energy_assets \n",
    "                                                if f'{asset}_ret_5d' in data.columns], axis=1).mean(axis=1)\n",
    "    \n",
    "    # Macro signals\n",
    "    if 'vix_close' in data.columns:\n",
    "        data['risk_on_signal'] = -data['vix_close'].pct_change(5)  # Lower VIX = risk on\n",
    "    \n",
    "    # Cross-asset correlations\n",
    "    return_cols = [col for col in data.columns if '_ret_1d' in col]\n",
    "    for i, col1 in enumerate(return_cols[:10]):  # Limit to avoid explosion\n",
    "        for col2 in return_cols[i+1:i+3]:\n",
    "            asset1 = col1.replace('_ret_1d', '')\n",
    "            asset2 = col2.replace('_ret_1d', '')\n",
    "            \n",
    "            for period in [5, 20]:\n",
    "                data[f'{asset1}_{asset2}_corr{period}'] = data[col1].rolling(period).corr(data[col2])\n",
    "    \n",
    "    # Calendar features\n",
    "    data['day_of_week'] = pd.to_datetime(data.index).dayofweek\n",
    "    data['month'] = pd.to_datetime(data.index).month\n",
    "    data['quarter'] = pd.to_datetime(data.index).quarter\n",
    "    \n",
    "    # Cyclical encoding\n",
    "    data['day_sin'] = np.sin(2 * np.pi * data['day_of_week'] / 7)\n",
    "    data['day_cos'] = np.cos(2 * np.pi * data['day_of_week'] / 7)\n",
    "    data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)\n",
    "    data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)\n",
    "    \n",
    "    # Get all new features\n",
    "    new_columns = set(data.columns) - original_columns\n",
    "    feature_columns = [col for col in new_columns \n",
    "                      if col not in ['next_day_return', 'next_day_volatility']]\n",
    "    \n",
    "    print(f\"   Created {len(feature_columns)} features dynamically\")\n",
    "    \n",
    "    # Select features and handle missing values\n",
    "    X = data[feature_columns].ffill().bfill().fillna(0)\n",
    "    X = X.replace([np.inf, -np.inf], [1e6, -1e6])\n",
    "    \n",
    "    # Targets\n",
    "    y_return = data['next_day_return']\n",
    "    y_volatility = data['next_day_volatility']\n",
    "    \n",
    "    # Create rank targets (quintiles)\n",
    "    y_rank = pd.qcut(y_return.dropna(), q=5, labels=[0, 1, 2, 3, 4])\n",
    "    \n",
    "    # Remove NaN targets\n",
    "    valid_idx = ~(y_return.isna() | y_volatility.isna())\n",
    "    X = X[valid_idx]\n",
    "    y_return = y_return[valid_idx]\n",
    "    y_volatility = y_volatility[valid_idx]\n",
    "    y_rank = y_rank[valid_idx]\n",
    "    \n",
    "    return X, y_return, y_volatility, y_rank\n",
    "\n",
    "# ============================================================================\n",
    "# ENHANCED MODEL WITH ATTENTION\n",
    "# ============================================================================\n",
    "\n",
    "class EnhancedTrajectoryGRU(nn.Module):\n",
    "    \"\"\"GRU with attention and sharper predictions\"\"\"\n",
    "    \n",
    "    def __init__(self, input_features, hidden_size=256, state_size=128, \n",
    "                 dropout=0.3, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.state_size = state_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Feature attention to identify important features\n",
    "        self.feature_attention = nn.Sequential(\n",
    "            nn.Linear(input_features, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, input_features),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(input_features + state_size, hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size * 2, hidden_size)\n",
    "        )\n",
    "        \n",
    "        # GRU with attention\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, \n",
    "                         batch_first=True, num_layers=3, dropout=dropout)\n",
    "        \n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_heads=n_heads, \n",
    "                                               dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # Prediction heads\n",
    "        self.return_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        self.volatility_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        \n",
    "        # Rank prediction head (5 classes)\n",
    "        self.rank_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 5)\n",
    "        )\n",
    "        \n",
    "        # Hidden state projection\n",
    "        self.state_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, state_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, hidden_state=None):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        if hidden_state is None:\n",
    "            hidden_state = torch.zeros(batch_size, self.state_size).to(x.device)\n",
    "        \n",
    "        # Apply feature attention\n",
    "        feature_weights = self.feature_attention(x)\n",
    "        x_weighted = x * feature_weights\n",
    "        \n",
    "        # Concatenate and project\n",
    "        x_combined = torch.cat([x_weighted, hidden_state], dim=-1)\n",
    "        x_projected = self.input_projection(x_combined)\n",
    "        x_projected = x_projected.unsqueeze(1)\n",
    "        \n",
    "        # GRU processing\n",
    "        gru_out, _ = self.gru(x_projected)\n",
    "        \n",
    "        # Self-attention\n",
    "        attn_out, attn_weights = self.attention(gru_out, gru_out, gru_out)\n",
    "        gru_out = gru_out + attn_out\n",
    "        gru_out = gru_out.squeeze(1)\n",
    "        \n",
    "        # Predictions\n",
    "        predicted_return = self.return_head(gru_out).squeeze(-1)\n",
    "        predicted_vol = self.volatility_head(gru_out).squeeze(-1)\n",
    "        rank_logits = self.rank_head(gru_out)\n",
    "        new_state = self.state_head(gru_out)\n",
    "        \n",
    "        return predicted_return, predicted_vol, rank_logits, new_state, feature_weights\n",
    "\n",
    "# ============================================================================\n",
    "# SHARPER LOSS FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "class SharperDistributionLoss(nn.Module):\n",
    "    \"\"\"Loss function that penalizes excessive uncertainty\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.0, beta=1.0, gamma=1.0, uncertainty_penalty=5.0, \n",
    "                 target_std=0.02, rank_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.uncertainty_penalty = uncertainty_penalty\n",
    "        self.target_std = target_std\n",
    "        self.rank_weight = rank_weight\n",
    "        \n",
    "    def forward(self, pred_mean, pred_std, rank_logits, actual_return, \n",
    "                realized_vol, rank_labels):\n",
    "        # Standard losses\n",
    "        return_loss = self.mse(pred_mean, actual_return)\n",
    "        vol_loss = self.mse(pred_std, realized_vol)\n",
    "        \n",
    "        # NLL loss\n",
    "        eps = 1e-6\n",
    "        variance = (pred_std + eps) ** 2\n",
    "        nll_loss = 0.5 * (torch.log(2 * np.pi * variance) + \n",
    "                         ((actual_return - pred_mean) ** 2) / variance)\n",
    "        nll_loss = nll_loss.mean()\n",
    "        \n",
    "        # Penalize excessive uncertainty\n",
    "        excess_uncertainty = torch.relu(pred_std - self.target_std)\n",
    "        uncertainty_loss = excess_uncertainty.mean() * self.uncertainty_penalty\n",
    "        \n",
    "        # Rank prediction loss\n",
    "        if rank_labels is not None and rank_logits is not None:\n",
    "            rank_loss = self.ce(rank_logits, rank_labels)\n",
    "        else:\n",
    "            rank_loss = 0\n",
    "        \n",
    "        total_loss = (self.alpha * return_loss + \n",
    "                     self.beta * vol_loss + \n",
    "                     self.gamma * nll_loss + \n",
    "                     uncertainty_loss +\n",
    "                     self.rank_weight * rank_loss)\n",
    "        \n",
    "        return total_loss, {\n",
    "            'return_mse': return_loss.item(),\n",
    "            'vol_mse': vol_loss.item(),\n",
    "            'nll': nll_loss.item(),\n",
    "            'uncertainty_penalty': uncertainty_loss.item(),\n",
    "            'rank_loss': rank_loss.item() if rank_loss != 0 else 0\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET\n",
    "# ============================================================================\n",
    "\n",
    "class EnhancedDataset(Dataset):\n",
    "    def __init__(self, features, returns, volatilities, ranks=None):\n",
    "        self.features = torch.FloatTensor(features.values if hasattr(features, 'values') else features)\n",
    "        self.returns = torch.FloatTensor(returns.values if hasattr(returns, 'values') else returns)\n",
    "        self.volatilities = torch.FloatTensor(volatilities.values if hasattr(volatilities, 'values') else volatilities)\n",
    "        if ranks is not None:\n",
    "            self.ranks = torch.LongTensor(ranks.values if hasattr(ranks, 'values') else ranks)\n",
    "        else:\n",
    "            self.ranks = None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.ranks is not None:\n",
    "            return self.features[idx], self.returns[idx], self.volatilities[idx], self.ranks[idx]\n",
    "        return self.features[idx], self.returns[idx], self.volatilities[idx], torch.tensor(0)\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING WITH LONGER EPOCHS\n",
    "# ============================================================================\n",
    "\n",
    "def train_enhanced_model(model, X_train, y_return_train, y_vol_train, y_rank_train,\n",
    "                         X_val, y_return_val, y_vol_val, y_rank_val,\n",
    "                         epochs=100, batch_size=32, lr=0.001, \n",
    "                         loss_weights=None):\n",
    "    \"\"\"Train with longer epochs and better optimization\"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    max_patience = 15\n",
    "    \n",
    "    # Default loss weights\n",
    "    if loss_weights is None:\n",
    "        loss_weights = {'alpha': 1.0, 'beta': 1.0, 'gamma': 1.0, \n",
    "                       'uncertainty_penalty': 5.0, 'rank_weight': 1.0}\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = EnhancedDataset(X_train, y_return_train, y_vol_train, y_rank_train)\n",
    "    val_dataset = EnhancedDataset(X_val, y_return_val, y_vol_val, y_rank_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Optimizer with different learning rates for different parts\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': model.feature_attention.parameters(), 'lr': lr * 2},\n",
    "        {'params': model.gru.parameters(), 'lr': lr},\n",
    "        {'params': model.return_head.parameters(), 'lr': lr},\n",
    "        {'params': model.volatility_head.parameters(), 'lr': lr},\n",
    "        {'params': model.rank_head.parameters(), 'lr': lr * 1.5}\n",
    "    ], weight_decay=1e-5)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5\n",
    "    )\n",
    "    \n",
    "    criterion = SharperDistributionLoss(**loss_weights)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_metrics = {'return_mse': 0, 'vol_mse': 0, 'nll': 0, \n",
    "                        'uncertainty_penalty': 0, 'rank_loss': 0}\n",
    "        epoch_train_loss = 0\n",
    "        \n",
    "        for features, returns, vols, ranks in train_loader:\n",
    "            features = features.to(device)\n",
    "            returns = returns.to(device)\n",
    "            vols = vols.to(device)\n",
    "            ranks = ranks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            pred_mean, pred_std, rank_logits, _, _ = model(features)\n",
    "            loss, metrics = criterion(pred_mean, pred_std, rank_logits, \n",
    "                                     returns, vols, ranks)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "            for key in train_metrics:\n",
    "                train_metrics[key] += metrics[key]\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_metrics = {'return_mse': 0, 'vol_mse': 0, 'nll': 0,\n",
    "                      'uncertainty_penalty': 0, 'rank_loss': 0}\n",
    "        epoch_val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, returns, vols, ranks in val_loader:\n",
    "                features = features.to(device)\n",
    "                returns = returns.to(device)\n",
    "                vols = vols.to(device)\n",
    "                ranks = ranks.to(device)\n",
    "                \n",
    "                pred_mean, pred_std, rank_logits, _, _ = model(features)\n",
    "                loss, metrics = criterion(pred_mean, pred_std, rank_logits,\n",
    "                                        returns, vols, ranks)\n",
    "                \n",
    "                epoch_val_loss += loss.item()\n",
    "                for key in val_metrics:\n",
    "                    val_metrics[key] += metrics[key]\n",
    "        \n",
    "        train_losses.append(epoch_train_loss / len(train_loader))\n",
    "        val_losses.append(epoch_val_loss / len(val_loader))\n",
    "        \n",
    "        scheduler.step(val_losses[-1])\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_losses[-1] < best_val_loss:\n",
    "            best_val_loss = val_losses[-1]\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= max_patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: Train Loss: {train_losses[-1]:.4f}, \"\n",
    "                  f\"Val Loss: {val_losses[-1]:.4f}\")\n",
    "            print(f\"  Return MSE: {val_metrics['return_mse']/len(val_loader):.4f}, \"\n",
    "                  f\"Rank Loss: {val_metrics['rank_loss']/len(val_loader):.4f}\")\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# ============================================================================\n",
    "# OPTUNA OPTIMIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def optimize_hyperparameters(X_train, y_return_train, y_vol_train, y_rank_train,\n",
    "                            X_val, y_return_val, y_vol_val, y_rank_val,\n",
    "                            n_trials=50):\n",
    "    \"\"\"Optimize hyperparameters using Optuna\"\"\"\n",
    "    \n",
    "    def objective(trial):\n",
    "        # Hyperparameters to optimize\n",
    "        hidden_size = trial.suggest_int('hidden_size', 128, 512, step=64)\n",
    "        state_size = trial.suggest_int('state_size', 64, 256, step=32)\n",
    "        dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "        lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "        n_heads = trial.suggest_categorical('n_heads', [4, 8, 16])\n",
    "        \n",
    "        # Loss function weights\n",
    "        alpha = trial.suggest_float('alpha_return', 0.5, 2.0)\n",
    "        beta = trial.suggest_float('beta_vol', 0.1, 1.0)\n",
    "        gamma = trial.suggest_float('gamma_nll', 0.1, 1.0)\n",
    "        uncertainty_penalty = trial.suggest_float('uncertainty_penalty', 1.0, 10.0)\n",
    "        rank_weight = trial.suggest_float('rank_weight', 0.5, 2.0)\n",
    "        target_std = trial.suggest_float('target_std', 0.01, 0.03)\n",
    "        \n",
    "        loss_weights = {\n",
    "            'alpha': alpha, 'beta': beta, 'gamma': gamma,\n",
    "            'uncertainty_penalty': uncertainty_penalty,\n",
    "            'rank_weight': rank_weight,\n",
    "            'target_std': target_std\n",
    "        }\n",
    "        \n",
    "        # Create and train model\n",
    "        model = EnhancedTrajectoryGRU(\n",
    "            input_features=X_train.shape[1],\n",
    "            hidden_size=hidden_size,\n",
    "            state_size=state_size,\n",
    "            dropout=dropout,\n",
    "            n_heads=n_heads\n",
    "        )\n",
    "        \n",
    "        # Train for fewer epochs during optimization\n",
    "        model, train_losses, val_losses = train_enhanced_model(\n",
    "            model, X_train, y_return_train, y_vol_train, y_rank_train,\n",
    "            X_val, y_return_val, y_vol_val, y_rank_val,\n",
    "            epochs=30, batch_size=batch_size, lr=lr,\n",
    "            loss_weights=loss_weights\n",
    "        )\n",
    "        \n",
    "        # Evaluate model sharpness\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_dataset = EnhancedDataset(X_val, y_return_val, y_vol_val, y_rank_val)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "            \n",
    "            all_stds = []\n",
    "            for features, _, _, _ in val_loader:\n",
    "                features = features.to(device)\n",
    "                _, pred_std, _, _, _ = model(features)\n",
    "                all_stds.append(pred_std.cpu().numpy())\n",
    "            \n",
    "            avg_std = np.concatenate(all_stds).mean()\n",
    "        \n",
    "        # Penalize models with too-wide confidence intervals\n",
    "        penalty = max(0, (avg_std - 0.025) * 100)\n",
    "        \n",
    "        return val_losses[-1] + penalty\n",
    "    \n",
    "    # Create study\n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
    "    )\n",
    "    \n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    \n",
    "    print(\"\\nBest hyperparameters:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    return study.best_params\n",
    "\n",
    "# ============================================================================\n",
    "# TRAJECTORY GENERATION\n",
    "# ============================================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_sharp_trajectories(model, initial_features, n_trajectories=1000, \n",
    "                               horizon=15, force_decision=True):\n",
    "    \"\"\"Generate trajectories with sharper, more decisive predictions\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    batch_size = 100\n",
    "    all_trajectories = []\n",
    "    all_ranks = []\n",
    "    \n",
    "    for batch_start in range(0, n_trajectories, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, n_trajectories)\n",
    "        current_batch_size = batch_end - batch_start\n",
    "        \n",
    "        features = initial_features.repeat(current_batch_size, 1).to(device)\n",
    "        hidden_state = None\n",
    "        batch_trajectories = []\n",
    "        batch_ranks = []\n",
    "        \n",
    "        cumulative_variance = torch.zeros(current_batch_size).to(device)\n",
    "        \n",
    "        for day in range(horizon):\n",
    "            pred_mean, pred_std, rank_logits, hidden_state, feature_weights = model(features, hidden_state)\n",
    "            \n",
    "            # Get rank predictions\n",
    "            rank_probs = F.softmax(rank_logits, dim=-1)\n",
    "            predicted_ranks = torch.argmax(rank_probs, dim=-1)\n",
    "            batch_ranks.append(predicted_ranks.cpu().numpy())\n",
    "            \n",
    "            if force_decision:\n",
    "                # Adjust mean based on rank prediction\n",
    "                rank_adjustment = torch.where(predicted_ranks == 0, -0.02,  # Strong sell\n",
    "                                            torch.where(predicted_ranks == 1, -0.01,  # Sell\n",
    "                                            torch.where(predicted_ranks == 2, 0.0,    # Hold\n",
    "                                            torch.where(predicted_ranks == 3, 0.01,   # Buy\n",
    "                                                       0.02))))  # Strong buy\n",
    "                pred_mean = pred_mean + rank_adjustment\n",
    "                \n",
    "                # Reduce std for more decisive predictions\n",
    "                pred_std = pred_std * 0.7\n",
    "            \n",
    "            # Add accumulated uncertainty (but less than before)\n",
    "            total_std = torch.sqrt(pred_std**2 + cumulative_variance * 0.5)\n",
    "            \n",
    "            # Sample returns\n",
    "            sampled_returns = torch.normal(pred_mean, total_std)\n",
    "            batch_trajectories.append(sampled_returns.cpu().numpy())\n",
    "            \n",
    "            # Accumulate variance with stronger decay\n",
    "            cumulative_variance = cumulative_variance * 0.9 + pred_std**2 * 0.3\n",
    "            \n",
    "            # Update features\n",
    "            features = features.clone()\n",
    "            features[:, 0] = sampled_returns\n",
    "            noise_scale = 0.005 * np.sqrt(day + 1)\n",
    "            features = features + torch.randn_like(features) * noise_scale\n",
    "        \n",
    "        all_trajectories.append(np.array(batch_trajectories).T)\n",
    "        all_ranks.append(np.array(batch_ranks).T)\n",
    "    \n",
    "    return np.vstack(all_trajectories), np.vstack(all_ranks)\n",
    "\n",
    "# ============================================================================\n",
    "# ENHANCED VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def plot_enhanced_results(trajectories, ranks, feature_importance=None):\n",
    "    \"\"\"Visualize enhanced model results\"\"\"\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Trajectory distribution\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    percentiles = {\n",
    "        'p5': np.percentile(trajectories, 5, axis=0),\n",
    "        'p25': np.percentile(trajectories, 25, axis=0),\n",
    "        'p50': np.percentile(trajectories, 50, axis=0),\n",
    "        'p75': np.percentile(trajectories, 75, axis=0),\n",
    "        'p95': np.percentile(trajectories, 95, axis=0)\n",
    "    }\n",
    "    days = np.arange(1, trajectories.shape[1] + 1)\n",
    "    \n",
    "    ax1.fill_between(days, percentiles['p5'], percentiles['p95'], \n",
    "                     alpha=0.2, color='blue', label='90% CI')\n",
    "    ax1.fill_between(days, percentiles['p25'], percentiles['p75'], \n",
    "                     alpha=0.3, color='blue', label='50% CI')\n",
    "    ax1.plot(days, percentiles['p50'], 'b-', linewidth=2, label='Median')\n",
    "    ax1.set_xlabel('Days Ahead')\n",
    "    ax1.set_ylabel('Cumulative Return')\n",
    "    ax1.set_title('Sharper Trajectory Predictions')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Rank distribution over time\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    rank_means = ranks.mean(axis=0)\n",
    "    rank_colors = ['darkred', 'red', 'gray', 'green', 'darkgreen']\n",
    "    \n",
    "    for rank in range(5):\n",
    "        rank_pct = (ranks == rank).mean(axis=0) * 100\n",
    "        ax2.plot(days, rank_pct, label=f'Rank {rank}', color=rank_colors[rank])\n",
    "    \n",
    "    ax2.set_xlabel('Days Ahead')\n",
    "    ax2.set_ylabel('Percentage')\n",
    "    ax2.set_title('Rank Distribution Over Time')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Feature importance (if available)\n",
    "    if feature_importance is not None:\n",
    "        ax3 = plt.subplot(2, 3, 3)\n",
    "        top_features = np.argsort(feature_importance)[-20:]\n",
    "        ax3.barh(range(20), feature_importance[top_features])\n",
    "        ax3.set_xlabel('Importance')\n",
    "        ax3.set_title('Top 20 Feature Importance')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Confidence interval width\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    ci_width = percentiles['p75'] - percentiles['p25']\n",
    "    ax4.plot(days, ci_width, 'g-', linewidth=2)\n",
    "    ax4.fill_between(days, 0, ci_width, alpha=0.3, color='green')\n",
    "    ax4.set_xlabel('Days Ahead')\n",
    "    ax4.set_ylabel('50% CI Width')\n",
    "    ax4.set_title('Prediction Confidence')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Return distribution at key horizons\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    horizons = [1, 5, 10, 15]\n",
    "    colors = ['blue', 'green', 'orange', 'red']\n",
    "    \n",
    "    for horizon, color in zip(horizons, colors):\n",
    "        if horizon <= trajectories.shape[1]:\n",
    "            ax5.hist(trajectories[:, horizon-1], bins=30, alpha=0.3,\n",
    "                    color=color, label=f'Day {horizon}', density=True)\n",
    "    \n",
    "    ax5.set_xlabel('Return')\n",
    "    ax5.set_ylabel('Density')\n",
    "    ax5.set_title('Return Distributions')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Decision signal strength\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    decision_strength = np.abs(rank_means - 2)  # Distance from neutral (rank 2)\n",
    "    ax6.plot(days, decision_strength, 'r-', linewidth=2)\n",
    "    ax6.fill_between(days, 0, decision_strength, alpha=0.3, color='red')\n",
    "    ax6.set_xlabel('Days Ahead')\n",
    "    ax6.set_ylabel('Signal Strength')\n",
    "    ax6.set_title('Decision Confidence')\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Enhanced Model Results with Sharper Predictions', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return percentiles\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def main(use_optuna=True):\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ENHANCED TRAJECTORY MODEL WITH SHARPER PREDICTIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\n1. Loading and preparing enhanced data...\")\n",
    "    X, y_return, y_volatility, y_rank = prepare_distribution_data()\n",
    "    print(f\"   Features: {X.shape[1]}\")\n",
    "    print(f\"   Samples: {len(X)}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_idx = int(0.8 * len(X))\n",
    "    val_idx = int(0.9 * len(X))\n",
    "    \n",
    "    X_train = X.iloc[:train_idx]\n",
    "    y_return_train = y_return.iloc[:train_idx]\n",
    "    y_vol_train = y_volatility.iloc[:train_idx]\n",
    "    y_rank_train = y_rank.iloc[:train_idx]\n",
    "    \n",
    "    X_val = X.iloc[train_idx:val_idx]\n",
    "    y_return_val = y_return.iloc[train_idx:val_idx]\n",
    "    y_vol_val = y_volatility.iloc[train_idx:val_idx]\n",
    "    y_rank_val = y_rank.iloc[train_idx:val_idx]\n",
    "    \n",
    "    X_test = X.iloc[val_idx:]\n",
    "    y_return_test = y_return.iloc[val_idx:]\n",
    "    y_vol_test = y_volatility.iloc[val_idx:]\n",
    "    y_rank_test = y_rank.iloc[val_idx:]\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Optuna optimization\n",
    "    if use_optuna:\n",
    "        print(\"\\n2. Running Optuna hyperparameter optimization...\")\n",
    "        best_params = optimize_hyperparameters(\n",
    "            X_train_scaled, y_return_train, y_vol_train, y_rank_train,\n",
    "            X_val_scaled, y_return_val, y_vol_val, y_rank_val,\n",
    "            n_trials=30\n",
    "        )\n",
    "    else:\n",
    "        best_params = {\n",
    "            'hidden_size': 256, 'state_size': 128, 'dropout': 0.3,\n",
    "            'lr': 0.001, 'batch_size': 32, 'n_heads': 8,\n",
    "            'alpha_return': 1.0, 'beta_vol': 0.5, 'gamma_nll': 0.5,\n",
    "            'uncertainty_penalty': 5.0, 'rank_weight': 1.0, 'target_std': 0.02\n",
    "        }\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    print(\"\\n3. Training final model with extended epochs...\")\n",
    "    model = EnhancedTrajectoryGRU(\n",
    "        input_features=X_train_scaled.shape[1],\n",
    "        hidden_size=best_params['hidden_size'],\n",
    "        state_size=best_params['state_size'],\n",
    "        dropout=best_params['dropout'],\n",
    "        n_heads=best_params['n_heads']\n",
    "    )\n",
    "    \n",
    "    loss_weights = {\n",
    "        'alpha': best_params['alpha_return'],\n",
    "        'beta': best_params['beta_vol'],\n",
    "        'gamma': best_params['gamma_nll'],\n",
    "        'uncertainty_penalty': best_params['uncertainty_penalty'],\n",
    "        'rank_weight': best_params['rank_weight'],\n",
    "        'target_std': best_params['target_std']\n",
    "    }\n",
    "    \n",
    "    model, train_losses, val_losses = train_enhanced_model(\n",
    "        model, X_train_scaled, y_return_train, y_vol_train, y_rank_train,\n",
    "        X_val_scaled, y_return_val, y_vol_val, y_rank_val,\n",
    "        epochs=100,  # Longer training\n",
    "        batch_size=best_params['batch_size'],\n",
    "        lr=best_params['lr'],\n",
    "        loss_weights=loss_weights\n",
    "    )\n",
    "    \n",
    "    # Generate sharp trajectories\n",
    "    print(\"\\n4. Generating sharp trajectory predictions...\")\n",
    "    test_features = torch.FloatTensor(X_test_scaled[0:1])\n",
    "    trajectories, ranks = generate_sharp_trajectories(\n",
    "        model, test_features, n_trajectories=1000, horizon=15,\n",
    "        force_decision=True\n",
    "    )\n",
    "    \n",
    "    # Get feature importance\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, _, _, _, feature_weights = model(test_features)\n",
    "        feature_importance = feature_weights[0].cpu().numpy()\n",
    "    \n",
    "    # Visualize results\n",
    "    print(\"\\n5. Plotting enhanced results...\")\n",
    "    percentiles = plot_enhanced_results(trajectories, ranks, feature_importance)\n",
    "    \n",
    "    # Price forecast\n",
    "    current_price = 36.50\n",
    "    cumulative_returns = np.cumsum(trajectories, axis=1)\n",
    "    median_prices = current_price * (1 + np.median(cumulative_returns, axis=0))\n",
    "    p25_prices = current_price * (1 + np.percentile(cumulative_returns, 25, axis=0))\n",
    "    p75_prices = current_price * (1 + np.percentile(cumulative_returns, 75, axis=0))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ENHANCED PRICE FORECAST (Sharper Predictions)\")\n",
    "    print(f\"Current Price: ${current_price:.2f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for day in [1, 5, 10, 15]:\n",
    "        if day <= len(median_prices):\n",
    "            rank_at_day = ranks[:, day-1].mean()\n",
    "            signal = ['STRONG SELL', 'SELL', 'HOLD', 'BUY', 'STRONG BUY'][int(rank_at_day)]\n",
    "            print(f\"Day {day:2d}: ${median_prices[day-1]:.2f} \"\n",
    "                  f\"[${p25_prices[day-1]:.2f} - ${p75_prices[day-1]:.2f}] \"\n",
    "                  f\"Signal: {signal}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Model complete with sharper, more decisive predictions!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return model, trajectories, ranks, feature_importance\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, trajectories, ranks, feature_importance = main(use_optuna=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "freelunch_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
